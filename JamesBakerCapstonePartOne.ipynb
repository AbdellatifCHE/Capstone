{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One - Clustering news articles according to inferred shared stories\n",
    "There are two parts to this project. The first part deals with takes a body of news articles and clustering them into stories. That part is covered in this workbook.\n",
    "The second part takes the results of the clustering (i.e. a list of articles pertaining to a single story) and ranks them in order to determine if the set of articles correspond to a balanced representation of the story, or a biased representation. This second part is covered in a second notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clarify some terms:\n",
    "- ARTICLE - a single article printed by one news publication\n",
    "- STORY - the underlying event that an article is in reference to\n",
    "\n",
    "In many instances the universe of publications will feature multiple articles on any one story. The questions were are ultimately seeking to address here are:\n",
    "- FURTHER READING - given that a user has read one article, which other articles should the user read in order to not have a biased perspective on the underlying event?\n",
    "- FAIRNESS - is the complete set of published articles on that story biased or fair?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workbook contains code required to:\n",
    "- load a large corpus of new articles\n",
    "- process them using a series of NLP methods\n",
    "- group the articles into inferred stories\n",
    "- analyse and graph the results\n",
    "- perform a grid search if required in order to optimise the hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "### Imports\n",
    "The first step is to import some of the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter configuration\n",
    "The parameters used to control the NLP-related calculations, and to specify the domain for any grid search are captured in the runParams dict. This includes specification of the location of the key input files.\n",
    "The runParams dict is converted into an sklearn ParameterGrid, even if there is no grid search requirement (in which case it's processed as a single scenario grid search).\n",
    "\n",
    "NB All parameters need to be lists (or lists of lists) - requirement of ParameterGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runParams={'tfidf_maxdf':      [0.5],\n",
    "           'input_file':       ['./data/articles.csv'],\n",
    "           'story_threshold':  [0.26],\n",
    "           'process_date':     ['2016-09-01'],\n",
    "           'parts_of_speech':  [['PROPER', 'VERB']],\n",
    "           'lemma_conversion': [False],\n",
    "           'ngram_max':        [3],\n",
    "           'tfidf_binary':     [False],\n",
    "           'tfidf_norm':       ['l2'],\n",
    "           'nlp_library':      ['nltk'],\n",
    "           'max_length':       [50],\n",
    "           'stop_words_file':  ['./data/stopWords.txt'],\n",
    "           'tfidf_mindf':      [2],\n",
    "           'display_graph':    [True],\n",
    "           'article_stats':    [False]}\n",
    "\n",
    "# Use parameter grid even if there is only set of parameters\n",
    "parameterGrid=ParameterGrid(runParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Libraries\n",
    "Two NLP libraries are used in this worksheet - ntkl and spaCy. Depending on which is/are requested in the run parameters, the following section of code loads the relevant packages. In addition it initialises a dictionary to translate a common set of Parts of Speech into the corresponding set of tokens specific to each library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and initialise required NLP libraries\n",
    "pos_nlp_mapping={}\n",
    "nl=None\n",
    "wordnet_lemmatizer=None\n",
    "nlp=None\n",
    "if 'spaCy' in runParams['nlp_library']:\n",
    "\timport spacy\n",
    "\tnlp=spacy.load('en')\n",
    "\tpos_nlp_mapping['spaCy']={'VERB':['VERB'],'PROPER':['PROPN'],'COMMON':['NOUN']}\n",
    "\n",
    "if 'nltk' in runParams['nlp_library']:\n",
    "\timport nltk as nl\n",
    "\tif True in runParams['lemma_conversion']:\n",
    "\t\tfrom nltk.stem import WordNetLemmatizer\n",
    "\t\twordnet_lemmatizer=WordNetLemmatizer()\n",
    "\telse:\n",
    "\t\twordnet_lemmatizer=None\n",
    "\tpos_nlp_mapping['nltk']={'VERB':['VB','VBD','VBG','VBN','VBP','VBZ'],'PROPER':['NNP','NNPS'],'COMMON':['NN','NNS']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File loader for news article corpus\n",
    "The following function loads the text file specified in the run parameters and converts it into a Pandas data frame.\n",
    "It proceeds to perform some clean up on the data, effectively removing articles that are in some sense or other corrupt and will not be able to processed by the algorithm.\n",
    "This set includes summary articles which effectively contain a single sentence on a large number of stories. It also removes some standardised text common to certain articles, since that text contains no information about the story itself and hence will create noise in the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on article dates\n",
    "The loader function below needs to be called with the selected articles to be constrained to a single date. This is because a key feature of \"news\" reporting is that it is current. As a result two articles are hugely more likely to pertain to the same story if they are published on the same day. Including the full set of dates has the effect of creating a lot of noise for the vectorizing and clustering, thus resulting in both slow performance and inaccurate results.\n",
    "There are additional techniques for pairing articles across different dates. These are discussed in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInputDataAndDisplayStats(filename,processDate,printSummary=False):\n",
    "\n",
    "\tdf=pd.read_csv(filename)\n",
    "\tdf=df.drop_duplicates('content')\n",
    "\tdf=df[~df['content'].isnull()]\n",
    "\n",
    "\t# There are a large number of junk articles, many of which either don't make sense or\n",
    "\t# just contain a headline - as such they are useless for this analysis and may distort\n",
    "\t# results if left in place\n",
    "\tdf=df[df['content'].str.len()>=200]\n",
    "\n",
    "\t# Find and remove summary NYT \"briefing\" articles to avoid confusing the clustering\n",
    "\ttargetString=\"(Want to get this briefing by email?\"\n",
    "\tdf['NYT summary']=df['content'].map(lambda d: d[:len(targetString)]==targetString)\n",
    "\tdf=df[df['NYT summary']==False]\n",
    "\n",
    "\t# The following removes a warning that appears in many of the Atlantic articles.\n",
    "\t# Since it is commonly at the beginning, it brings a lot of noise to the search for similar articles\n",
    "\t# And subsequently to the assessment of sentiment\n",
    "\ttargetString=\"For us to continue writing great stories, we need to display ads.             Please select the extension that is blocking ads.     Please follow the steps below\"\n",
    "\tdf['content']=df['content'].str.replace(targetString,'')\n",
    "\n",
    "\t# This is also for some Atlantic articles for the same reasons as above\n",
    "\ttargetString=\"This article is part of a feature we also send out via email as The Atlantic Daily, a newsletter with stories, ideas, and images from The Atlantic, written specially for subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "\tdf=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\t# This is also for some Atlantic articles for the same reasons as above\n",
    "\ttargetString=\"This article is part of a feature we also send out via email as Politics  Policy Daily, a daily roundup of events and ideas in American politics written specially for newsletter subscribers. To sign up, please enter your email address in the field provided here.\"\n",
    "\tdf=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\t# More Atlantic-specific removals (for daily summaries with multiple stories contained)\n",
    "\tdf=df[df['content'].str.contains(\"To sign up, please enter your email address in the field\")==False]\n",
    "\n",
    "\t# Remove daily CNN summary\n",
    "\ttargetString=\"CNN Student News\"\n",
    "\tdf=df[df['content'].str.contains(targetString)==False]\n",
    "\n",
    "\tif printSummary:\n",
    "\t\tprint(\"\\nArticle counts by publisher:\")\n",
    "\t\tprint(df['publication'].value_counts())\n",
    "\n",
    "\t\tprint(\"\\nArticle counts by date:\")\n",
    "\t\tprint(df['date'].value_counts())\n",
    "\t\t\n",
    "\t# Restrict to articles on the provided input date.\n",
    "\t# This date is considered mandatory for topic clustering but is not required for sentiment\n",
    "\t# since sentiment only processes a specified list of articles.\n",
    "\t# For topic clustering it is essential to have the date as it is\n",
    "\t# enormously significant in article matching.\n",
    "\tif processDate!=None:\n",
    "\t\tdf=df[df['date']==processDate]\n",
    "\tdf.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\t# Remove non-ASCII characters\n",
    "\tdf['content no nonascii']=df['content'].map(lambda x: removeNonASCIICharacters(x))\n",
    "\n",
    "\tprint(\"\\nFinal dataset:\\n\\nDate:\",processDate,\"\\n\")\n",
    "\tprint(df['publication'].value_counts())\n",
    "\n",
    "\treturn df\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def removeNonASCIICharacters(textString): \n",
    "    return \"\".join(i for i in textString if ord(i)<128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the articles from the corpus\n",
    "In addition the function will return the number of articles per publication (for the requested run date). Here we see there is a relatively good mix of political viewpoints covered. More discussion of this is provided in the project report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corpus of articles from file\n",
    "# 0 index is required because the parameters are forced to be lists by ParameterGrid\n",
    "articleDataFrame=getInputDataAndDisplayStats(runParams['input_file'][0],\n",
    "\t\t\t\t\t\t\t\t\t\t\t runParams['process_date'][0],\n",
    "\t\t\t\t\t\t\t\t\t\t\t runParams['article_stats'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect loaded articles\n",
    "Now that the articles are loaded, the only attributes that will be used are the ID and the 'content no non-ascii' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(articleDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect full article corpus\n",
    "Although the remaining set of data are not used, for the purpose of exploring the dataset, the following breakdown can be obtained. Note that at the bottom of the list are some misformatted dates. These are not pertinent to the date being used for the example here, so it is not necessary to address that at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getInputDataAndDisplayStats(runParams['input_file'][0],\n",
    "                            runParams['process_date'][0],\n",
    "                            printSummary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Processing\n",
    "### Stop words\n",
    "In processing natural language, it is necessary to suppress words that convey little value. Typically these are common words such as \"a\", \"man\", \"Friday\", etc. They are referred to as Stop Words. A list of these files is in an included file and is loaded below. This file is independent of whether nltk or spaCy is used for some of the other NLP features. (it will only be applied towards the end of the NLP processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStopWords(stopWordsFileName):\n",
    "\tstop_words=[]\n",
    "\tf=open(stopWordsFileName, 'r')\n",
    "\tfor l in f.readlines():\n",
    "\t\tstop_words.append(l.replace('\\n', ''))\n",
    "\treturn stop_words\n",
    "\n",
    "\n",
    "# Load stop words now - these will be deleted from final text by processor before vectorizing\n",
    "# 0 index is required because the parameters are forced to be lists by ParameterGrid\n",
    "stop_words=loadStopWords(runParams['stop_words_file'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "The following function provides the NLTK pre-processing. Specifically, it:\n",
    "- restricts the text to the requested parts-of-speech (according to the run parameters). This includes Proper Nouns, Common Nouns, Verbs, etc. Words of different parts-of-speech are more/less important in determining the story relayed by an article - for example, adjectives are not important. The goal is to reduce the number of words in a way that eliminates those that cause noise rather than adding value - and thus makes the algorithm operate more effectively.\n",
    "- applies lemmatisation (optionally), thus substituting a word for its root - the intention here being to reduce the final universe of words in the corpus, and thus make finding related articles easier.\n",
    "- truncates the length of the article to the degree requested. (testing has shown that restricting to the first few paragraph increases the ease with which the algorithm can relate articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringNLTKProcess(nl,stringToConvert,partsOfSpeech,stop_words,maxWords=None,lemmatizer=None):\n",
    "\tsentences=nl.sent_tokenize(stringToConvert)\n",
    "\tstr=[]\n",
    "\tfor sentence in sentences:\n",
    "\t\twordString=[]\n",
    "\t\tfor word,pos in nl.pos_tag(nl.word_tokenize(sentence)):\n",
    "\t\t\t# The following condition avoids any POS which corresponds to punctuation (and takes all others)\n",
    "\t\t\tif partsOfSpeech==None:\n",
    "\t\t\t\tif pos[0]>='A' and pos[0]<='Z':\n",
    "\t\t\t\t\twordString.append(word)\n",
    "\t\t\telif pos in partsOfSpeech:\n",
    "\t\t\t\twordString.append(word)\n",
    "\t\tfor wrd in wordString:\n",
    "\t\t\twrdlower=wrd.lower()\n",
    "\t\t\tif wrdlower not in stop_words and wrdlower!=\"'s\":\n",
    "\t\t\t\tif maxWords==None or len(str)<maxWords:\n",
    "\t\t\t\t\tif lemmatizer==None:\n",
    "\t\t\t\t\t\tstr.append(wrdlower)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tstr.append(lemmatizer.lemmatize(wrd.lower(), pos='v'))\n",
    "\t\t\tif maxWords!=None and len(str)==maxWords:\n",
    "\t\t\t\treturn ' '.join(str)\n",
    "\treturn ' '.join(str)\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def removeSpacesAndPunctuation(textString): \n",
    "    return \"\".join(i for i in textString if (ord(i)>=48 and ord(i)<=57) or (ord(i)>=97 and ord(i)<=122))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "The second NLP library supported here is spaCy. It is being used to provide the same features as NLTK - although there are pros and cons to the specific implementations of each library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringSpaCyProcess(nlp,stringToConvert,partsOfSpeech,maxWords,stop_words,lemmatize):\n",
    "\tdoc=nlp(stringToConvert)\n",
    "\tif partsOfSpeech==None:\n",
    "\t\tspacyTokens=[w for w in doc]\n",
    "\telse:\n",
    "\t\tspacyTokens=[w for w in doc if w.pos_ in partsOfSpeech]\n",
    "\n",
    "\tstr=[]\n",
    "\tfor spt in spacyTokens:\n",
    "\t\tif lemmatize:\n",
    "\t\t\twrd=spt.lemma_\n",
    "\t\telse:\n",
    "\t\t\twrd=spt.text\n",
    "\t\twrdlower=removeSpacesAndPunctuation(wrd.lower())\n",
    "\t\t# The middle term below is correctly wrd.lower() not wrdlower since the function call\n",
    "\t\t# above strips out the --, and I don't want to compare with 'pron' in case that\n",
    "\t\t# finds false matches\n",
    "\t\tif wrdlower not in stop_words and wrd.lower()!='-pron-' and not wrdlower=='':\n",
    "\t\t\tif maxWords==None or len(str)<maxWords:\n",
    "\t\t\t\tstr.append(wrdlower)\n",
    "\t\tif maxWords!=None and len(str)==maxWords:\n",
    "\t\t\t\treturn ' '.join(str)\t\t\n",
    "\treturn ' '.join(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for analysing results\n",
    "In order to score how well the algorithm is assigning articles to stories, it is useful (but optional) to provide a file containing a \"story map\". This file effectively specifies which articles belong to which stories. It is incomplete, but it is sufficiently extensive to demonstrate the effectiveness of the results.\n",
    "In addition, a list of article IDs can be provided to drive the post vectorization validation. For the purposes of this workbook, this second list is being derived from the story map.\n",
    "### Setup story map and testing list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupStoryMapAndReportList(args=None,reportArticleList=None,storyMapFileName=None):\n",
    "\t# Story Map is used in fitting if grid search is applied (As ground truth)\n",
    "\t# It is also used in graph if no threshold provided (to determine colours, not to determine location)\n",
    "\t# Report Article List is used at the end to create a report with, for each\n",
    "\t# article in the list, the set of articles within tolerance, and the key words for each\n",
    "\tif args==None:\n",
    "\t\tarticleList=reportArticleList\n",
    "\t\tfileName=storyMapFileName\n",
    "\telse:\n",
    "\t\tarticleList=args['article_id_list']\n",
    "\t\tfileName=args['story_map_validation']\n",
    "\n",
    "\treportArticleList=articleList\n",
    "\tif fileName!=None:\n",
    "\t\tstoryMap=readStoryMapFromFile(fileName)\n",
    "\t\tif reportArticleList==None:\n",
    "\t\t\treportArticleList=[]\n",
    "\t\t\tfor story, articleList in storyMap.items():\n",
    "\t\t\t\treportArticleList.append(articleList[0])\n",
    "\telse:\n",
    "\t\tstoryMap=None\n",
    "\treturn storyMap,reportArticleList\n",
    "\n",
    "def readStoryMapFromFile(filename):\n",
    "\treturn readDictFromCsvFile(filename,'StoryMap')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def readGridParameterRangeFromFile(filename):\n",
    "\treturn readDictFromCsvFile(filename,'GridParameters')\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def readDictFromCsvFile(filename,schema):\n",
    "\tgridParamDict={}\n",
    "\twith open(filename, 'r') as f:\n",
    "\t\tfor row in f:\n",
    "\t\t\trow=row[:-1] # Exclude the carriage return\n",
    "\t\t\trow=row.split(\",\")\n",
    "\t\t\tkey=row[0]\n",
    "\t\t\tvals=row[1:]\n",
    "\t\t\t\n",
    "\t\t\tif schema=='GridParameters':\n",
    "\t\t\t\tif key in ['story_threshold','tfidf_maxdf']:\n",
    "\t\t\t\t\tfinalVals=list(float(n) for n in vals)\n",
    "\t\t\t\telif key in ['ngram_max','tfidf_mindf','max_length']:\n",
    "\t\t\t\t\tfinalVals=list(int(n) for n in vals)\n",
    "\t\t\t\telif key in ['lemma_conversion','tfidf_binary']:\n",
    "\t\t\t\t\tfinalVals=list(str2bool(n) for n in vals)\n",
    "\t\t\t\telif key in ['parts_of_speech']:\n",
    "\t\t\t\t\tlistlist=[]\n",
    "\t\t\t\t\tfor v in vals:\n",
    "\t\t\t\t\t\tlistlist.append(v.split(\"+\"))\n",
    "\t\t\t\t\tfinalVals=listlist\n",
    "\t\t\t\telif key in ['tfidf_norm','nlp_library']:\n",
    "\t\t\t\t\tfinalVals=vals\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(key)\n",
    "\t\t\t\t\tprint(\"KEY ERROR\")\n",
    "\t\t\t\t\treturn\n",
    "\t\t\telif schema=='StoryMap':\n",
    "\t\t\t\tfinalVals=list(int(n) for n in vals)\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint(schema)\n",
    "\t\t\t\tprint(\"SCHEMA ERROR\")\n",
    "\t\t\t\treturn\n",
    "\t\t\t\n",
    "\t\t\tgridParamDict[key]=finalVals\n",
    "\treturn gridParamDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the story map from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storyMap,reportArticleList=setupStoryMapAndReportList(storyMapFileName='storyMapForValidation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the story map we see that it forms a dict containing a key corresponding to the name of the story and a value containing a list of the article IDs germane to that story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for story, articleList in storyMap.items():\n",
    "    print(story,\":\",articleList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(reportArticleList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm preparation\n",
    "The algorithm can be considered as having the following steps:\n",
    "- Preprocessing for NLP features\n",
    "- Conversion to TF-IDF vectors\n",
    "- A Relatedness Score for a set of vectors\n",
    "- A main processing loop for tying everything together\n",
    "\n",
    "### Preprocess and vectorizing\n",
    "This section provides the necessary code for the natural language processing requirements.\n",
    "\n",
    "Two NLP libraries are supported. The user can choose (in the run parameters) between:\n",
    "- NLTK\n",
    "- SpaCy\n",
    "\n",
    "The various natural language functions will be applied (to the extent requested in the run parameters).\n",
    "- Lemmatization\n",
    "- Remove stop words\n",
    "- Restrict to specific parts-of-speech\n",
    "- Constrain overall length\n",
    "- n-grams\n",
    "\n",
    "Once that's done, the body of the processed articles will be analysed and converted into tf-idf values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessAndVectorize(articleDataFrame,args,pos_nlp_mapping,nlp,nl,wordnet_lemmatizer,stop_words):\n",
    "\t# Map the input parts of speech list to the coding required for the specific NLP library\n",
    "\tif args['parts_of_speech'][0]!='ALL':\n",
    "\t\tpartsOfSpeech=[]\n",
    "\t\tfor pos in args['parts_of_speech']:\n",
    "\t\t\tpartsOfSpeech.append(pos_nlp_mapping[args['nlp_library']][pos])\n",
    "\t\tpartsOfSpeech=[item for sublist in partsOfSpeech for item in sublist]\n",
    "\telse:\n",
    "\t\tpartsOfSpeech=None\n",
    "\n",
    "\t# Processing of text depends on NLP library choice\n",
    "\tif args['nlp_library']=='spaCy':\n",
    "\t\tarticleDataFrame['input to vectorizer']=articleDataFrame['content no nonascii'].map(lambda x: stringSpaCyProcess(nlp,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   x,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   partsOfSpeech=partsOfSpeech,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   maxWords=args['max_length'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   stop_words=stop_words,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   lemmatize=args['lemma_conversion']))\n",
    "\telif args['nlp_library']=='nltk':\n",
    "\t\tarticleDataFrame['input to vectorizer']=articleDataFrame['content no nonascii'].map(lambda x: stringNLTKProcess(nl,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  x,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  partsOfSpeech=partsOfSpeech,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  stop_words=stop_words,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  maxWords=args['max_length'],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  lemmatizer=wordnet_lemmatizer))\n",
    "\telse:\n",
    "\t\tprint(\"PROBLEM... NO VALID NLP LIBRARY... MUST BE nltk OR spaCy\")\n",
    "\n",
    "\t# To get default values a couple of parameters need to be not passed if not specified on the command line\n",
    "\t# Passing as None behaves differently to passing no parameter (which would invoke the default value)\n",
    "\toptArgsForVectorizer={}\n",
    "\tif args['tfidf_maxdf'] != None:\n",
    "\t\toptArgsForVectorizer['max_df']=args['tfidf_maxdf']\n",
    "\tif args['tfidf_mindf'] != None:\n",
    "\t\toptArgsForVectorizer['min_df']=args['tfidf_mindf']\n",
    "\n",
    "\t# Create and run the vectorizer\n",
    "\tvectorizer=TfidfVectorizer(analyzer='word',\n",
    "   \t    \t                   ngram_range=(1,args['ngram_max']),\n",
    "       \t    \t               lowercase=True,\n",
    "           \t    \t    \t   binary=args['tfidf_binary'],\n",
    "               \t\t    \t   norm=args['tfidf_norm'],\n",
    "\t\t\t\t\t\t\t   **optArgsForVectorizer)\n",
    "\ttfidfVectors=vectorizer.fit_transform(articleDataFrame['input to vectorizer'])\n",
    "\tterms=vectorizer.get_feature_names()\n",
    "\treturn tfidfVectors, terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring\n",
    "Scores must be computed for each pair of articles, for the following reasons:\n",
    "- To determine the proposed clustering of articles in stories (and to evaluate this clustering against a ground truth story map)\n",
    "- To evaluate the grid parameters in order to choose the preferable combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,threshold,printErrors=False):\n",
    "\t# Work with distances relative to first item in each cluster - even though this is clearly arbitrary since that\n",
    "\t# point could be an outlier in the cluster and hence might cause problems.\n",
    "\t# But I have to start somewhere - and can refine it later if needed.\n",
    "\n",
    "\tnonZeroCoords=initialiseAllNonZeroCoords(tfidfVectors)\n",
    "\tscore=0\n",
    "\toutGood=0\n",
    "\toutBad=0\n",
    "\tinGood=0\n",
    "\tinBad=0\n",
    "\tfor story, storyArticles in storyMap.items():\n",
    "\t\tleadArticleIndex=articleDataFrame[articleDataFrame['id']==storyArticles[0]].index[0]\n",
    "\t\t# Compute score of all articles in corpus relative to first article in story (.product)\n",
    "\t\t# Then count through list relative to threshold (add one for a good result, subtract one for a bad result)\n",
    "\t\tscores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "\t\trankedIndices=np.argsort(scores)\n",
    "\t\tfoundRelatedArticles=[]\n",
    "\t\t# THE SORTING HERE IS NOT STRICTLY REQUIRED, BUT I COULD USE IT SO THAT ONCE THE THRESHOLD IS PASSED\n",
    "\t\t# IN THE LOOP, THEN I INFER THE REMAINING RESULTS\n",
    "\t\tfor article in reversed(rankedIndices):\n",
    "\t\t\tthisArticleIndex=articleDataFrame['id'][article]\n",
    "\t\t\tif thisArticleIndex in storyArticles:\n",
    "\t\t\t\tif scores[article]>=threshold:\n",
    "\t\t\t\t\tscore+=1\n",
    "\t\t\t\t\tinGood+=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tscore-=1\n",
    "\t\t\t\t\tinBad+=1\n",
    "\t\t\t\t\tif printErrors:\n",
    "\t\t\t\t\t\tprint(\"ERROR:\",thisArticleIndex,\"should be in\",story)\n",
    "\t\t\telse: # article not supposed to be in range\n",
    "\t\t\t\tif scores[article]<=threshold:\n",
    "\t\t\t\t\tscore+=1\n",
    "\t\t\t\t\toutGood+=1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tscore-=1\n",
    "\t\t\t\t\toutBad+=1\n",
    "\t\t\t\t\tif printErrors:\n",
    "\t\t\t\t\t\tprint(\"ERROR:\",thisArticleIndex,\"should NOT be in\",story)\n",
    "\tscoreDict={'score':score,'inGood':inGood,'inBad':inBad,'outGood':outGood,'outBad':outBad}\n",
    "\treturn scoreDict\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "def initialiseAllNonZeroCoords(tfidfVectors):\n",
    "# This function just exists since it seems to be expensive and I'd rather not call it multiple times\n",
    "# Hence it is intended to be called outside of loops in order to simplify the row specific processing\n",
    "\tvalues=[]\n",
    "\tnzc=zip(*tfidfVectors.nonzero())\n",
    "\n",
    "\t# In Python 3 the zip can only be iterated through one time before it is automatically released\n",
    "\t# So need to copy the results otherwise the main loop below will no longer work\n",
    "\tpointList=[]\n",
    "\tfor i,j in nzc:\n",
    "\t\tpointList.append([i,j])\t\t\n",
    "\n",
    "\tfor row in range(tfidfVectors.shape[0]):\n",
    "\t\trowList=[]\n",
    "\t\tfor i,j in pointList:\n",
    "\t\t\tif row==i:\n",
    "\t\t\t\trowList.append(j)\n",
    "\t\tvalues.append(rowList)\n",
    "\n",
    "\treturn values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relatedness Scoring measure\n",
    "The Relatedness Score is computed between a pair of articles by taking the dot product of the values across each dimension of the pair's TF-IDF vectors.\n",
    "This has the following behaviour characteristics:\n",
    "- If a term is important in both articles, that term will have a high impact on the article relatedness\n",
    "- If a term is not important in either or both articles, that term will have a high impact on the article relatedness\n",
    "\n",
    "The non-linearity coming from the product ensures a more contextual and intuitive scoring than the conventional Euclidean measure. (thus resulting in more usable pairings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productRelatednessScores(tfidfVectors,nonZeroCoords,refRow):\n",
    "\tscores=[0]*tfidfVectors.shape[0]\n",
    "\tfor toRow in range(tfidfVectors.shape[0]):\n",
    "\t\tscores[toRow]=sum([(tfidfVectors[toRow,w]*tfidfVectors[refRow,w]) for w in nonZeroCoords[refRow] if w in nonZeroCoords[toRow]])\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the algorithm\n",
    "Now that all the pieces are in place, a loop is run to tie everything together - calling the vectorizer and scoring the results.\n",
    "If we are running in GridSearch mode, the loop will repeat and keep track of the best results achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop across all parameter combinations in grid to determine best set\n",
    "# If not doing grid search, will just pass through the loop once\n",
    "bestParamScoreDict={'score':-1000000}\n",
    "bestParams=parameterGrid[0]\n",
    "for i,currentParams in enumerate(parameterGrid):\n",
    "\tif len(parameterGrid)>1:\n",
    "\t\tprint(\"Combination:\",i+1,\"of\",len(parameterGrid))\n",
    "\t\tprint(currentParams)\n",
    "\n",
    "\t# Determine tf-idf vectors\n",
    "\t# terms is just used later on if analysis of final results is requested\n",
    "\ttfidfVectors,terms=preprocessAndVectorize(articleDataFrame,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  currentParams,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  pos_nlp_mapping,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nlp,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nl,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  wordnet_lemmatizer,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  stop_words)\n",
    "\n",
    "\t# Compute scores if threshold provided (meaning as part of grid search)\n",
    "\tif 'story_threshold' in currentParams and currentParams['story_threshold']!=None:\n",
    "\t\tscoreDict=scoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,currentParams['story_threshold'])\n",
    "\t\tprint(scoreDict)\n",
    "\n",
    "\t\t# Update best so far\n",
    "\t\tif scoreDict['score']>=bestParamScoreDict['score']:\n",
    "\t\t\tif len(parameterGrid)>1:\n",
    "\t\t\t\tprint(i+1,\"is the best so far!\")\n",
    "\t\t\tbestParams=currentParams\n",
    "\t\t\tbestParamScoreDict=scoreDict\n",
    "\t# End grid/parameter loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy up by restoring to best run before proceeding with analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold to input value from best (and possibly only) run for use in results analysis\n",
    "# Unless not specified at all\n",
    "if 'story_threshold' in bestParams and bestParams['story_threshold']!=None:\n",
    "\tthreshold=bestParams['story_threshold']\n",
    "else:\n",
    "\tthreshold=None\n",
    "\n",
    "\n",
    "# If there was a real parameter grid, then output/refresh results\n",
    "if len(parameterGrid)>1:\n",
    "\tprint(\"BEST PARAMETERS:\")\n",
    "\tprint(bestParams)\n",
    "\tprint(bestParamScoreDict)\n",
    "\tscoreCurrentParamGuess(tfidfVectors,storyMap,articleDataFrame,threshold,printErrors=True)\n",
    "\t# Recreate vector for best results in loop\n",
    "\t# terms is just used later on if analysis of final results is requested\n",
    "\ttfidfVectors,terms=preprocessAndVectorize(articleDataFrame,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  bestParams,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  pos_nlp_mapping,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nlp,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  nl,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  wordnet_lemmatizer,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of results\n",
    "### Produce graphs\n",
    "In order to produce a graph of the results, the TF-IDF vectors are reduced to two dimensions.\n",
    "\n",
    "Clustering is computed using the full n dimensions, with the threshold determining which articles end up grouped into shared stories.\n",
    "\n",
    "The graphs is ultimately rendered using Bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce vector space to two dimensions\n",
    "# Then produce Bokeh graph\n",
    "def graphVectorSpace(tfidfVectors,extraColumns,dateForTitle,storyMap,threshold):\n",
    "\t# Better results seem to be obtained by breaking the dimensionality reduction into two steps\n",
    "\n",
    "\t# First reduce to fifty dimensions with SVD\n",
    "\tfrom sklearn.decomposition import TruncatedSVD\n",
    "\tsvd=TruncatedSVD(n_components=50, random_state=0)\n",
    "\tsvdResults=svd.fit_transform(tfidfVectors)\n",
    "\n",
    "\t# Next continue to two dimensions with TSNE\n",
    "\tfrom sklearn.manifold import TSNE\n",
    "\ttsneModel=TSNE(n_components=2, verbose=0, random_state=0, n_iter=500)\n",
    "\ttsneResults=tsneModel.fit_transform(svdResults)\n",
    "\ttfidf2dDataFrame=pd.DataFrame(tsneResults)\n",
    "\ttfidf2dDataFrame.columns=['x','y']\n",
    "\n",
    "\ttfidf2dDataFrame['publication']=extraColumns['publication']\t\n",
    "\ttfidf2dDataFrame['id']=extraColumns['id']\t\n",
    "\ttfidf2dDataFrame['content']=extraColumns['content no nonascii'].map(lambda x: x[:200])\n",
    "\n",
    "\t# All articles will be marked as NA to indicate that they have not been assigned to a story\n",
    "\t# Then those which have been assigned one will be updated to refer to that\n",
    "\ttfidf2dDataFrame['category']='NA'\n",
    "\n",
    "\t# If the threshold is not provided, then just graph the vector space as is\n",
    "\t# With colours indicating desired story grouping\n",
    "\t# This still has value because it shows how well stories cluster together\n",
    "\tif threshold==None:\n",
    "\t\tgraphTitle=(\"TF-IDF article clustering - story assignment from map - \"+dateForTitle[0])\n",
    "\t\tfor story, storyArticles in storyMap.items():\n",
    "\t\t\tfor article in storyArticles:\n",
    "\t\t\t\tif len(tfidf2dDataFrame[tfidf2dDataFrame['id']==article].index)==1:\n",
    "\t\t\t\t\ti=tfidf2dDataFrame[tfidf2dDataFrame['id']==article].index[0]\n",
    "\t\t\t\t\ttfidf2dDataFrame['category'][i]=story\n",
    "\telse:\n",
    "\t\tgraphTitle=(\"TF-IDF article clustering - story assignment computed - \"+dateForTitle[0])\n",
    "\t\tnonZeroCoords=initialiseAllNonZeroCoords(tfidfVectors)\n",
    "\t\tfor story, storyArticles in storyMap.items():\n",
    "\t\t\tleadArticleIndex=extraColumns[extraColumns['id']==storyArticles[0]].index[0]\n",
    "\t\t\t# Compute score of all articles in corpus relative to first article in story (.product)\n",
    "\t\t\tscores=productRelatednessScores(tfidfVectors,nonZeroCoords,leadArticleIndex)\n",
    "\t\t\trankedIndices=np.argsort(scores)\n",
    "\t\t\tfor article in rankedIndices:\n",
    "\t\t\t\tif scores[article]>=threshold:\n",
    "\t\t\t\t\ttfidf2dDataFrame['category'][article]=story\n",
    "\n",
    "\timport bokeh.plotting as bp\n",
    "\tfrom bokeh.models import HoverTool\n",
    "\tfrom bokeh.plotting import show,output_notebook\n",
    "\tfrom bokeh.palettes import d3\n",
    "\timport bokeh.models as bmo\n",
    "\n",
    "\toutput_notebook()\n",
    "\tplot_tfidf=bp.figure(plot_width=800, plot_height=800, title=graphTitle,\n",
    "\t\t\t\t\t\t tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "\t\t\t\t\t\t x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "\tnumCats=len(tfidf2dDataFrame['category'].unique())\n",
    "\tpalette=d3['Category20'][numCats]\n",
    "\tcolor_map=bmo.CategoricalColorMapper(factors=tfidf2dDataFrame['category'].map(str).unique(), palette=palette)\n",
    "\n",
    "\tplot_tfidf.scatter(x='x', y='y', color={'field': 'category', 'transform': color_map}, \n",
    "\t\t\t\t\t\tlegend='category',source=tfidf2dDataFrame)\n",
    "\thover=plot_tfidf.select(dict(type=HoverTool))\n",
    "\tplot_tfidf.legend.click_policy=\"hide\"\n",
    "\thover.tooltips={\"id\": \"@id\", \"publication\": \"@publication\", \"content\":\"@content\", \"category\":\"@category\"}\n",
    "\n",
    "\tshow(plot_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the graph\n",
    "Each dot on the scattergraph corresponds to an article.\n",
    "Most of the stories covered that day (and available in the dataset) are actually not represented in more than one publication. So many articles are effectively their own unique story - these are indicated by NA.\n",
    "There is a particularly large cluster of these around the center of the graph. This appears to be an artefact of the SVD - the articles don't contain strong distinctive terms, and hence have small values on both axes. (this is likely party a result of the min DF being set to 2 in the example)\n",
    "\n",
    "To view the details of any story, hover over the corresponding dot.\n",
    "\n",
    "Considerably more detail and analysis is provided in the project report document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphVectorSpace(tfidfVectors,\n",
    "\t\t\t\t articleDataFrame[['id','publication','content no nonascii']],\n",
    "\t\t\t\t runParams['process_date'],\n",
    "\t\t\t\t storyMap,\n",
    "\t\t\t\t threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the avoidance of doubt - recall that the story names in the legend (Safe space, Trump meeting, etc) are taken directly from the input story map titles. These names are not inferred from the data! (just the groupings are inferred)\n",
    "### Investigate inferred clustering vs given story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produceRequestedReportDetails(tfidfVectors,articleDataFrame,reportArticleList,threshold,storyMap,terms):\n",
    "\n",
    "\t# tfidfVectors is a sparse matrix, for efficiency it's useful to determine once only which\n",
    "\t# coordinates have non-zero values\n",
    "\tnonZeroCoords=initialiseAllNonZeroCoords(tfidfVectors)\n",
    "\n",
    "\ttopNwords=25\n",
    "\n",
    "\t# Create list of articles to process\n",
    "\t# If a list is provided in command line arguments, use that\n",
    "\tstoryMapGood=0.0\n",
    "\tencounteredStoriesList=[]\n",
    "\tfor index,row in articleDataFrame.iterrows():\n",
    "\t\tif row['id'] in reportArticleList:\n",
    "\t\t\tref_index=index\n",
    "\t\t\tprint(\"-----\")\n",
    "\t\t\tprint(\"-----\")\n",
    "\t\t\tprint(\"LEAD ARTICLE IN STORY:\",row['id'])\n",
    "\t\t\tprint(\"-----\")\n",
    "\n",
    "\t\t\tif threshold==None:\n",
    "\t\t\t\tarticleIndexList=[index]\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Score and rank all articles relative to this one\n",
    "\t\t\t\t# Count number of items that are greater than or equal to threshold\n",
    "\t\t\t\t# Then truncate the list beyond those items\n",
    "\t\t\t\tscores=productRelatednessScores(tfidfVectors,nonZeroCoords,ref_index)\n",
    "\t\t\t\trankedIndices=np.argsort(scores)\n",
    "\t\t\t\tnumItemsInRange=sum(x>=threshold for x in scores)\n",
    "\t\t\t\tarticleIndexList=rankedIndices[-numItemsInRange:]\n",
    "\n",
    "\t\t\t# If there is a story map, find out which story this article is meant to belong to\n",
    "\t\t\ttargetStory=None\n",
    "\t\t\tif storyMap!=None:\n",
    "\t\t\t\tfor story,articleList in storyMap.items():\n",
    "\t\t\t\t\tif row['id'] in articleList:\n",
    "\t\t\t\t\t\ttargetStory=story\n",
    "\t\t\t\t\t\ttargetArticleList=articleList\n",
    "\t\t\t\t\t\tencounteredStoriesList.append(targetStory)\n",
    "\t\t\t\t\t\n",
    "\t\t\t# For just those articles that are within threshold of the lead article\n",
    "\t\t\t# Print out the key terms and their tf-idf scores\n",
    "\t\t\t# Then count the number of articles that are correctly assigned to the story\n",
    "\t\t\t# (if there is a ground truth storyMap provided)\n",
    "\t\t\tfor article in reversed(articleIndexList):\n",
    "\t\t\t\tif targetStory!=None:\n",
    "\t\t\t\t\t# If this is officially part of the same story, update the counts\n",
    "\t\t\t\t\tif articleDataFrame['id'][article] in targetArticleList:\n",
    "\t\t\t\t\t\tstoryMapGood+=1.0\n",
    "\n",
    "\t\t\t\tprint(\"MEMBER ARTICLE:\",articleDataFrame['id'][article])\n",
    "\t\t\t\tif threshold!=None:\n",
    "\t\t\t\t\tprint(\"Score :\",scores[article])\n",
    "\t\t\t\tprint(articleDataFrame['publication'][article])\n",
    "\t\t\t\tprint(articleDataFrame['content'][article][:500])\n",
    "\t\t\t\tprint(\"PASSED TO VECTORIZER AS:\")\n",
    "\t\t\t\tprint(articleDataFrame['input to vectorizer'][article])\n",
    "\t\t\t\tprint()\n",
    "\t\t\t\tprintTopNwordsForArticle(tfidfVectors,terms,articleNum=article,n=topNwords)\n",
    "\t\t\t\tprint(\"-----\")\n",
    "\t\t\tprint(\"-----\")\n",
    "\n",
    "\t# If there is a storyMap, print out the percentage results for the inferred allocation\n",
    "\t# Note that it should be just relative to the number of stories actually encountered\n",
    "\t# So if the user requests a specific set of articles and those articles don't cover\n",
    "\t# the full set of stories, then they shouldn't be counted as errors.\n",
    "\tif storyMap!=None:\n",
    "\t\tstoryMapSize=sum([len(storyMap[story]) for story in encounteredStoriesList])\n",
    "\t\tprint(\"\\n\\nPERCENTAGE OF STORIES ALLOCATED IN LINE WITH MAP:\",100.*storyMapGood/storyMapSize)\n",
    "\n",
    "\treturn\n",
    "\n",
    "def printTopNwordsForArticle(tfidfVectors,terms,articleNum,n):\n",
    "\tvect=tfidfVectors[articleNum].toarray()[0]\n",
    "\ttopn1=np.argsort(vect)\n",
    "\tfor t in reversed(topn1[-n:]):\n",
    "\t\tif vect[t]>0.001:\n",
    "\t\t\tprint(terms[t],\":\",round(vect[t],5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering report for articles\n",
    "The output of the following cell is broken down as follows:\n",
    "- One section per story in the story map\n",
    "- The ID of the lead story (which is taken from the reportArticleList provided\n",
    "- For each article which has a pair score with this lead article greater than the threshold:\n",
    " - The ID of that related article\n",
    " - The score for the pair\n",
    " - The name of the publisher of the article\n",
    " - The first few lines of the actual article content\n",
    " - The derived corresponding text that is provided to the TF-IDF vectorizer (i.e. processed for lemmatization, parts-of-speech, stop words, etc)\n",
    " - The 25 most significant terms (in TF-IDF) in that article, along with that term's correspondong TF-IDF value for the article\n",
    "At the very end of the report is a line which indicates the percentage of articles which were allocated to the correct stories (according to the story map)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with outputting from best results if requested\n",
    "produceRequestedReportDetails(tfidfVectors,articleDataFrame,reportArticleList,threshold,storyMap,terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Capstone)",
   "language": "python",
   "name": "capstonepython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
